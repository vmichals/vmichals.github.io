<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta name="description" content="Academic homepage of Vincent Michalski">
    <meta name="author" content="Vincent Michalski">
    <meta name="keywords" content="Neural networks,Recurrent neural networks,computer vision,deep learning,theano">
    <title>Vincent Michalski</title>
</head>
<body>
  <p><em>(Old homepage as placeholder. Will be replaced soon.)</em></p>

I am currently a PhD candidate at the <a href="http://www.mila.umontreal.ca">Montreal Institute for Learning Algorithms (MILA)</a>. 
My advisors are <a href="http://www.iro.umontreal.ca/~vincentp" target=_blank>Pascal Vincent</a> and <a href="http://cs.mcgill.ca/~dprecup" target=_blank>Doina Precup</a>. 
My research focus is on architectures for visual reasoning, modeling of visual time series using recurrent neural networks and model-based reinforcement learning.

<ul>
    <li><a href="#contact">Email</a></li>
    <li><a href=https://scholar.google.com/citations?user=9BGzHdUAAAAJ>Google Scholar</a></li>
    <li><a href="https://github.com/vmichals">GitHub</a></li>
    <li><a href=https://www.linkedin.com/in/vincent-michalski-87930a12b>LinkedIn</a></li>
</ul>

<a name="pubs" />
<h2>Publications</h2>
<h3>2018</h3>
<ul>
    <li>2018 Li, R., Ebrahimi Kahou, S., Schulz, H., Michalski, V., Pal, C., Charlin, L.<br />
        <strong>Towards Deep Conversational Recommendations</strong><br />
        Neural Information Processing Systems (<a href="https://neurips.cc/Conferences/2018">NeurIPS 2018</a>)<br />
    </li>
    <li>2018 Sharma, S., Suhubdy, D., Michalski, V., Ebrahimi Kahou, S., Bengio, Y.<br />
        <strong>ChatPainter: Improving Text to Image Generation using Dialogue</strong><br />
        International Conference on Learning Representations Workshop (<a href="https://iclr.cc/Conferences/2018">ICLRW 2018</a>)<br />
        <a href="https://arxiv.org/pdf/1802.08216">[pdf]</a>
    </li>
    <li>2018 Ebrahimi Kahou, S.*, Michalski, V.*, Atkinson, A., Kádár, Á., Trischler, A., Bengio, Y.<br />
        <strong>FigureQA: An annotated figure dataset for visual reasoning</strong><br />
        International Conference on Learning Representations Workshop (<a href="https://iclr.cc/Conferences/2018">ICLRW 2018</a>)<br />
        <a href="https://arxiv.org/pdf/1710.07300">[pdf]</a><a href="https://datasets.maluuba.com/FigureQA">[website]</a>
    </li>
</ul>
<h3>2017</h3>
<ul>
    <li>2017 Serban, I., Sankar, C., Germain, M., Zhang, S., Lin, Z., Subramanian, S., Kim, T., Pieper, M., Chandar, S., Ke, N., Mudumba, S., de Brebisson, A., Sotelo, J., Suhubdy, D., Michalski, V., Nguyen, A., Pineau, J., Bengio, Y.<br />
        <strong>A Deep Reinforcement Learning Chatbot</strong><br />
        arXiv preprint arXiv:1709.02349<br />
        <a href="https://arxiv.org/pdf/1709.02349">[pdf]</a>
    </li>
    <li>2017 Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzyńska, J. et al.<br />
        <strong>The "something something" video database for learning and evaluating visual common sense</strong><br />
        International Conference On Computer Vision 2017 (ICCV 2017)<br />
        <a href="https://arxiv.org/pdf/1706.04261">[pdf]</a><a href="https://www.twentybn.com/datasets/something-something">[website]</a>
    </li>
    <li>2017 Kahou, S. E., Michalski, V., Memisevic, R., Pal, C., Vincent, P.<br />
    <strong>RATM: Recurrent Attentive Tracking Model</strong><br />
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops<br />
    <a href="https://arxiv.org/pdf/1510.08660">[pdf]</a> (arXiv preprint arXiv:1510.08660)</li>
</ul>
<h3>2016</h3>
<ul>
    <li>2016 The Theano Development Team<br />
        <strong>Theano: A Python framework for fast computation of mathematical expressions</strong><br />
        <a href="https://arxiv.org/pdf/1605.02688">[pdf]</a> (arXiv preprint arXiv:1605.02688)</li>
</ul>
<h3>2015</h3>
<ul>
    <li>2015 Kahou, S. E., Michalski, V., Konda, K., Memisevic, R., Pal, C.<br />
    <strong>Recurrent Neural Networks for Emotion Recognition in Video</strong><br />
    In proceedings of the 17th ACM International Conference on Multimodal Interaction (ICMI '15)
    <br />
    <a href="pdf/emotion_rnns.pdf">[pdf]</a><a href="bibtex/emornn.bib">[bibtex]</a><a href="https://github.com/saebrahimi/Emotion-Recognition-RNN">[code]</a></li>
    <li>2015 Kahou, S. E., Bouthillier, X., Lamblin, P., Gulcehre, C., Michalski, V., Konda, K., Jean, S., Froumenty, P., Courville, A., Vincent, P., Memisevic, R., Pal, C., Bengio, Y.<br />
    <strong>EmoNets: Multimodal deep learning approaches for emotion recognition in video</strong><br />
    In Journal on Multimodal User Interfaces, doi:10.1007/s12193-015-0195-2<br />
    <a href="https://arxiv.org/pdf/1503.01800.pdf">[pdf]</a> (arXiv preprint arXiv:1503.01800)<a href="bibtex/emonets.bib">[bibtex]</a></li>
</ul>
<h3>2014</h3>
<ul class="publist">
    <li>2014 Michalski, V., Memisevic, R., Konda, K.<br />
    <strong>Modeling Deep Temporal Dependencies with Recurrent&#8221;Grammar Cells&#8221;</strong><br />
    Neural Information Processing Systems (<a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014">NIPS 2014</a>)<br />
    <a title="Modeling Deep Temporal Dependencies with Recurrent “Grammar Cells”" href="https://papers.nips.cc/paper/5549-modeling-deep-temporal-dependencies-with-recurrent-grammar-cells.pdf">[pdf]</a>
    <a title="Modeling Deep Temporal Dependencies with Recurrent “Grammar Cells” - Supplementary Material" href="https://papers.nips.cc/paper/5549-modeling-deep-temporal-dependencies-with-recurrent-grammar-cells-supplemental.zip">[supplementary]</a><!--
    <a href="todo">[webpage]</a>
    --><a href="https://papers.nips.cc/paper/5549-modeling-deep-temporal-dependencies-with-recurrent-grammar-cells/bibtex">[bibtex]</a></li>
    <li>2014 Konda, K., Memisevic, R., Michalski, V.<br />
    <b>Learning to encode motion using spatio-temporal synchrony</b><br />
    International Conference on Learning Representations (<a href="https://sites.google.com/site/representationlearning2014/">ICLR 2014</a>)<br />
    <a href="https://arxiv.org/pdf/1306.3162">[pdf]</a><a href="bibtex/synchrony.bib">[bibtex]</a></li>
</ul>
<h3>2013</h3>
<ul>
    <li>2013 Konda K, Memisevic R, Michalski V<br />
    <strong>Boltzmann machines with dendritic gating.</strong><br />
    <i>Bernstein Conference 2013</i>. doi: <a href="https://portal.g-node.org/abstracts/bc13/#/doi/nncn.bc2013.0160">10.12751/nncn.bc2013.0160</a></li>
</ul>

<a name="contact" />
<h2>Contact</h2>
Email: [lastname][firstname without last two letters][at]gmail[dot]com
</body>
</html>
